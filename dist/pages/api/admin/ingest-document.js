"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.default = handler;
const geminiProcessor_1 = require("@/utils/geminiProcessor");
const embeddingClient_js_1 = require("@/utils/embeddingClient.js");
const errorHandling_1 = require("@/utils/errorHandling");
const adminWorkflow_1 = require("@/utils/adminWorkflow");
const documentProcessing_js_1 = require("@/utils/documentProcessing.js");
/**
 * API endpoint for ingesting documents with Gemini processing
 * This endpoint processes documents, analyzes them with Gemini, and adds them to the pending queue
 * Enhanced with contextual chunking for improved document understanding
 */
async function handler(req, res) {
    // Only allow POST requests
    if (req.method !== 'POST') {
        return res.status(405).json({ message: 'Method not allowed' });
    }
    try {
        // Extract document text and metadata from request
        const { text, source, existingMetadata, useEnhancedLabeling = true, // Default to enhanced labeling 
        useContextualChunking = true, // Added flag for contextual chunking
        retryCount = 0 // For handling Gemini processing retries
         } = req.body;
        if (!text || typeof text !== 'string') {
            return res.status(400).json({ message: 'Missing or invalid document text' });
        }
        if (!source || typeof source !== 'string') {
            return res.status(400).json({ message: 'Missing document source identifier' });
        }
        // Log ingestion request
        (0, errorHandling_1.logInfo)('Processing document for ingestion', {
            source,
            textLength: text.length,
            hasExistingMetadata: !!existingMetadata,
            useEnhancedLabeling,
            useContextualChunking
        });
        // Check for potential content conflicts early
        const { hasConflicts, conflictingDocIds } = await (0, adminWorkflow_1.checkForContentConflicts)({ source, ...(existingMetadata || {}) }, text);
        // Process document with Gemini - always try enhanced processing first
        let metadata;
        let analysisSummary;
        let processingSuccess = true;
        try {
            // Use enhanced processing with more detailed categories and labels
            const enhancedAnalysis = await (0, geminiProcessor_1.processDocumentWithEnhancedLabels)(text);
            // Validate minimum required fields exist in the analysis
            if (!enhancedAnalysis.primaryCategory ||
                !enhancedAnalysis.summary ||
                enhancedAnalysis.technicalLevel === undefined) {
                throw new Error('Insufficient metadata generated by Gemini');
            }
            metadata = {
                ...(0, geminiProcessor_1.convertEnhancedAnalysisToMetadata)(enhancedAnalysis),
                // Add source ID and other provided metadata
                source,
                ...(existingMetadata || {})
            };
            // Return enhanced analysis summary
            analysisSummary = {
                summary: enhancedAnalysis.summary,
                contentType: enhancedAnalysis.contentType,
                primaryCategory: enhancedAnalysis.primaryCategory,
                technicalLevel: enhancedAnalysis.technicalLevel,
                complexityScore: enhancedAnalysis.complexityScore,
                keyEntities: {
                    people: enhancedAnalysis.entities.people.map(p => p.name),
                    companies: enhancedAnalysis.entities.companies.map(c => c.name)
                },
                keywords: enhancedAnalysis.keywords,
                semanticKeywords: enhancedAnalysis.semanticKeywords,
                topics: enhancedAnalysis.topics,
                subtopics: enhancedAnalysis.subtopics,
                confidenceScore: enhancedAnalysis.confidenceScore,
                authorityScore: enhancedAnalysis.authorityScore,
                industries: enhancedAnalysis.industryCategories,
                useCases: enhancedAnalysis.useCases
            };
        }
        catch (enhancedError) {
            // Log the enhanced processing error
            (0, errorHandling_1.logError)('Enhanced Gemini processing failed', enhancedError);
            // Fall back to standard processing if enhanced fails
            try {
                const standardAnalysis = await (0, geminiProcessor_1.processDocumentWithGemini)(text);
                metadata = {
                    ...(0, geminiProcessor_1.convertAnalysisToMetadata)(standardAnalysis),
                    source,
                    ...(existingMetadata || {})
                };
                // Return standard analysis summary
                analysisSummary = {
                    summary: standardAnalysis.summary,
                    contentType: standardAnalysis.contentType,
                    primaryCategory: standardAnalysis.primaryCategory,
                    technicalLevel: standardAnalysis.technicalLevel,
                    keyEntities: {
                        people: standardAnalysis.entities.people.map(p => p.name),
                        companies: standardAnalysis.entities.companies.map(c => c.name)
                    },
                    keywords: standardAnalysis.keywords,
                    confidenceScore: standardAnalysis.confidenceScore
                };
                // Mark that we had to fall back
                processingSuccess = false;
            }
            catch (standardError) {
                // If both enhanced and standard processing fail, retry once more
                if (retryCount < 1) {
                    // Retry the request once with backoff
                    (0, errorHandling_1.logInfo)('Retrying Gemini processing after failure', { source, retryCount });
                    // Make a new request with incremented retry count
                    const retryResponse = await fetch('/api/admin/ingest-document', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({
                            text,
                            source,
                            existingMetadata,
                            useEnhancedLabeling: true,
                            useContextualChunking,
                            retryCount: retryCount + 1
                        })
                    });
                    return res.status(retryResponse.status).json(await retryResponse.json());
                }
                // If retries exhausted, use minimal metadata
                (0, errorHandling_1.logError)('Both enhanced and standard Gemini processing failed', standardError);
                metadata = {
                    source,
                    ...(existingMetadata || {}),
                    summary: text.substring(0, 200) + '...',
                    primaryCategory: 'general',
                    technicalLevel: 2,
                    processingFailed: true
                };
                analysisSummary = {
                    summary: text.substring(0, 200) + '...',
                    contentType: 'unknown',
                    primaryCategory: 'general',
                    technicalLevel: 2,
                    keyEntities: { people: [], companies: [] },
                    keywords: [],
                    confidenceScore: 0
                };
                processingSuccess = false;
            }
        }
        // Process document with contextual chunking if enabled
        let chunks = [];
        if (useContextualChunking) {
            try {
                // Use our new contextual chunking with the document metadata
                (0, errorHandling_1.logInfo)('Applying contextual chunking to document', { source });
                chunks = await (0, documentProcessing_js_1.splitIntoChunksWithContext)(text, 500, source, true, metadata);
                (0, errorHandling_1.logInfo)(`Created ${chunks.length} contextual chunks`, { source });
            }
            catch (chunkingError) {
                (0, errorHandling_1.logError)('Error in contextual chunking', chunkingError);
                // Fall back to non-contextual processing
                (0, errorHandling_1.logInfo)('Falling back to standard chunking', { source });
                // We'll handle this in the addToPendingDocuments function
            }
        }
        // Generate embedding for vector storage (if we haven't done contextual chunking)
        const embedding = chunks.length === 0 ? await (0, embeddingClient_js_1.embedText)(text) : null;
        // Add to pending documents queue instead of vector store
        // Pass the chunks if we have them, otherwise null to use default processing
        const documentId = await (0, adminWorkflow_1.addToPendingDocuments)(text, metadata, embedding, chunks.length > 0 ? chunks : null);
        // Return success with analysis summary and conflict information
        return res.status(200).json({
            success: true,
            message: 'Document processed and added to pending queue',
            documentId,
            requiresApproval: true,
            useEnhancedLabeling,
            useContextualChunking,
            chunksCreated: chunks.length,
            processingSuccess,
            hasConflicts,
            conflictingDocIds: hasConflicts ? conflictingDocIds : [],
            analysis: analysisSummary
        });
    }
    catch (error) {
        // Log error
        (0, errorHandling_1.logError)('Error processing document for ingestion', error);
        // Return error response
        return res.status(500).json({
            success: false,
            message: 'Failed to process document',
            error: error instanceof Error ? error.message : 'Unknown error'
        });
    }
}
