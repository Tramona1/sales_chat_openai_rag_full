"use strict";
/**
 * Migration Script for Contextual Embeddings
 *
 * This script migrates existing documents in the vector store to include
 * contextual information generated by Gemini.
 */
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.migrateToContextualEmbeddings = migrateToContextualEmbeddings;
const fs_1 = __importDefault(require("fs"));
const path_1 = __importDefault(require("path"));
const dotenv_1 = __importDefault(require("dotenv"));
const geminiClient_js_1 = require("../utils/geminiClient.js");
const embeddingClient_js_1 = require("../utils/embeddingClient.js");
const errorHandling_js_1 = require("../utils/errorHandling.js");
// Load environment variables
dotenv_1.default.config({ path: path_1.default.resolve(process.cwd(), '.env.local') });
// Constants
const VECTOR_STORE_PATH = path_1.default.join(process.cwd(), 'data', 'vectorStore.json');
const BATCH_DIR = path_1.default.join(process.cwd(), 'data', 'vector_batches');
const MIGRATION_LOG_PATH = path_1.default.join(process.cwd(), 'data', 'migration_log.json');
// Configuration
const DEFAULT_BATCH_SIZE = 10;
const SIMULATE = false; // Set to true to simulate without writing changes
// Function to create initial progress tracking
function initializeProgress() {
    return {
        startedAt: new Date().toISOString(),
        totalDocuments: 0,
        processedDocuments: 0,
        enhancedDocuments: 0,
        errorDocuments: 0,
        batches: []
    };
}
// Function to update progress
function updateProgress(progress) {
    fs_1.default.writeFileSync(MIGRATION_LOG_PATH, JSON.stringify(progress, null, 2));
}
// Function to get document batches
function getDocumentBatches() {
    // Check if batch directory exists
    if (!fs_1.default.existsSync(BATCH_DIR)) {
        return [];
    }
    // Get all JSON files in the batch directory
    const files = fs_1.default.readdirSync(BATCH_DIR);
    return files
        .filter(file => file.startsWith('batch_') && file.endsWith('.json'))
        .map(file => path_1.default.join(BATCH_DIR, file));
}
// Function to load documents from a batch file
function loadBatchDocuments(batchPath) {
    try {
        const batchData = fs_1.default.readFileSync(batchPath, 'utf8');
        return JSON.parse(batchData);
    }
    catch (error) {
        (0, errorHandling_js_1.logError)(`Error loading batch from ${batchPath}`, error);
        return [];
    }
}
// Function to generate context for a document chunk
async function enhanceChunkWithContext(chunk, documentContext = null) {
    var _a;
    try {
        // Skip if already has context
        if (((_a = chunk.metadata) === null || _a === void 0 ? void 0 : _a.context) &&
            chunk.metadata.documentSummary &&
            chunk.metadata.isContextualChunk) {
            return chunk;
        }
        // Create a new metadata object if it doesn't exist
        if (!chunk.metadata) {
            chunk.metadata = {};
        }
        // Add document context if available
        if (documentContext) {
            chunk.metadata.documentSummary = documentContext.summary;
            chunk.metadata.documentType = documentContext.documentType;
            chunk.metadata.primaryTopics = documentContext.mainTopics.join(', ');
            chunk.metadata.audienceType = documentContext.audienceType.join(', ');
            chunk.metadata.technicalLevel = documentContext.technicalLevel;
        }
        // Generate context for this specific chunk
        const chunkContext = await (0, geminiClient_js_1.generateChunkContext)(chunk.text, documentContext);
        // Add chunk context to metadata
        chunk.metadata.context = chunkContext;
        chunk.metadata.isContextualChunk = true;
        return chunk;
    }
    catch (error) {
        (0, errorHandling_js_1.logError)('Error enhancing chunk with context', error);
        return chunk;
    }
}
// Group chunks by source document
function groupChunksBySource(chunks) {
    var _a;
    const groupedChunks = {};
    for (const chunk of chunks) {
        const source = ((_a = chunk.metadata) === null || _a === void 0 ? void 0 : _a.source) || 'unknown';
        if (!groupedChunks[source]) {
            groupedChunks[source] = [];
        }
        groupedChunks[source].push(chunk);
    }
    return groupedChunks;
}
// Main migration function
async function migrateToContextualEmbeddings(batchSize = DEFAULT_BATCH_SIZE) {
    var _a;
    console.log('Starting migration to contextual embeddings...');
    // Check for Gemini API key
    if (!process.env.GEMINI_API_KEY) {
        console.error('Error: GEMINI_API_KEY is not set in .env.local');
        process.exit(1);
    }
    // Initialize progress tracking
    let progress;
    if (fs_1.default.existsSync(MIGRATION_LOG_PATH)) {
        // Resume from existing progress
        const progressData = fs_1.default.readFileSync(MIGRATION_LOG_PATH, 'utf8');
        progress = JSON.parse(progressData);
        console.log('Resuming migration from previous progress...');
    }
    else {
        // Start new migration
        progress = initializeProgress();
        updateProgress(progress);
    }
    // Initialize embedding client
    const embeddingClient = (0, embeddingClient_js_1.getEmbeddingClient)();
    console.log(`Using embedding provider: ${embeddingClient.getProvider()}`);
    try {
        // Process the single vector store file if it exists
        if (fs_1.default.existsSync(VECTOR_STORE_PATH)) {
            const data = fs_1.default.readFileSync(VECTOR_STORE_PATH, 'utf8');
            let vectorStore = JSON.parse(data);
            // Handle different vector store formats
            let items = [];
            if (vectorStore.items && Array.isArray(vectorStore.items)) {
                items = vectorStore.items;
            }
            else if (Array.isArray(vectorStore)) {
                items = vectorStore;
            }
            progress.totalDocuments += items.length;
            console.log(`Processing ${items.length} documents from main vector store...`);
            // Group chunks by source document
            const groupedChunks = groupChunksBySource(items);
            // Process each source document
            for (const [source, chunks] of Object.entries(groupedChunks)) {
                console.log(`Processing document: ${source} with ${chunks.length} chunks`);
                try {
                    // Build a representative document text for context extraction
                    // Combine first chunks up to a reasonable size
                    const maxContextSize = 10000;
                    let combinedText = '';
                    let contentAdded = false;
                    for (const chunk of chunks) {
                        if (combinedText.length < maxContextSize) {
                            combinedText += chunk.text + '\n\n';
                            contentAdded = true;
                        }
                    }
                    if (!contentAdded) {
                        console.log(`Skipping empty document: ${source}`);
                        continue;
                    }
                    // Extract document context
                    console.log(`Extracting context for document: ${source}`);
                    const documentContext = await (0, geminiClient_js_1.extractDocumentContext)(combinedText);
                    console.log(`Context extracted for ${source}: ${documentContext.summary.substring(0, 100)}...`);
                    // Process chunks in batches
                    const enhancedChunks = [];
                    for (let i = 0; i < chunks.length; i += batchSize) {
                        const batchChunks = chunks.slice(i, i + batchSize);
                        console.log(`Processing batch ${Math.floor(i / batchSize) + 1}/${Math.ceil(chunks.length / batchSize)}`);
                        // Process chunks in parallel
                        const enhancedBatch = await Promise.all(batchChunks.map(chunk => enhanceChunkWithContext(chunk, documentContext)));
                        enhancedChunks.push(...enhancedBatch);
                        // Update progress
                        progress.processedDocuments += batchChunks.length;
                        progress.enhancedDocuments += enhancedBatch.filter(c => { var _a; return (_a = c.metadata) === null || _a === void 0 ? void 0 : _a.isContextualChunk; }).length;
                        updateProgress(progress);
                    }
                    // Replace the original chunks with enhanced ones if not simulating
                    if (!SIMULATE) {
                        // Find the index positions of the chunks from this source
                        for (let i = 0; i < items.length; i++) {
                            if (((_a = items[i].metadata) === null || _a === void 0 ? void 0 : _a.source) === source) {
                                // Find the corresponding enhanced chunk
                                const enhancedChunk = enhancedChunks.find(c => {
                                    var _a, _b, _c, _d;
                                    return ((_a = c.metadata) === null || _a === void 0 ? void 0 : _a.id) === ((_b = items[i].metadata) === null || _b === void 0 ? void 0 : _b.id) ||
                                        ((_c = c.metadata) === null || _c === void 0 ? void 0 : _c.chunkId) === ((_d = items[i].metadata) === null || _d === void 0 ? void 0 : _d.chunkId);
                                });
                                if (enhancedChunk) {
                                    items[i] = enhancedChunk;
                                }
                            }
                        }
                    }
                }
                catch (error) {
                    (0, errorHandling_js_1.logError)(`Error processing document: ${source}`, error);
                    progress.errorDocuments += chunks.length;
                    updateProgress(progress);
                }
            }
            // Save updated vector store if not simulating
            if (!SIMULATE) {
                if (vectorStore.items) {
                    vectorStore.items = items;
                }
                else {
                    vectorStore = items;
                }
                // Add last updated timestamp
                if (typeof vectorStore === 'object') {
                    vectorStore.lastUpdated = Date.now();
                }
                fs_1.default.writeFileSync(VECTOR_STORE_PATH, JSON.stringify(vectorStore, null, 2));
                console.log(`Updated main vector store with contextual embeddings`);
            }
        }
        // Process batch files if they exist
        const batchPaths = getDocumentBatches();
        if (batchPaths.length > 0) {
            console.log(`Found ${batchPaths.length} batch files to process`);
            // Add batch information to progress tracking
            for (const batchPath of batchPaths) {
                const batchId = path_1.default.basename(batchPath, '.json');
                // Skip if already in progress tracking
                if (!progress.batches.some(b => b.batchId === batchId)) {
                    const documents = loadBatchDocuments(batchPath);
                    progress.totalDocuments += documents.length;
                    progress.batches.push({
                        batchId,
                        documentsCount: documents.length,
                        processedCount: 0,
                        status: 'pending'
                    });
                }
            }
            updateProgress(progress);
            // Process each batch file
            for (const batchPath of batchPaths) {
                const batchId = path_1.default.basename(batchPath, '.json');
                const batchProgress = progress.batches.find(b => b.batchId === batchId);
                // Skip completed batches
                if ((batchProgress === null || batchProgress === void 0 ? void 0 : batchProgress.status) === 'completed') {
                    console.log(`Skipping already completed batch: ${batchId}`);
                    continue;
                }
                console.log(`Processing batch: ${batchId}`);
                try {
                    // Update batch status
                    if (batchProgress) {
                        batchProgress.status = 'in-progress';
                        batchProgress.startedAt = new Date().toISOString();
                        updateProgress(progress);
                    }
                    // Load batch documents
                    const documents = loadBatchDocuments(batchPath);
                    console.log(`Loaded ${documents.length} documents from batch ${batchId}`);
                    // Group chunks by source document
                    const groupedChunks = groupChunksBySource(documents);
                    // Process each source document
                    for (const [source, chunks] of Object.entries(groupedChunks)) {
                        console.log(`Processing document: ${source} with ${chunks.length} chunks`);
                        try {
                            // Build a representative document text for context extraction
                            const maxContextSize = 10000;
                            let combinedText = '';
                            let contentAdded = false;
                            for (const chunk of chunks) {
                                if (combinedText.length < maxContextSize) {
                                    combinedText += chunk.text + '\n\n';
                                    contentAdded = true;
                                }
                            }
                            if (!contentAdded) {
                                console.log(`Skipping empty document: ${source}`);
                                continue;
                            }
                            // Extract document context
                            console.log(`Extracting context for document: ${source}`);
                            const documentContext = await (0, geminiClient_js_1.extractDocumentContext)(combinedText);
                            console.log(`Context extracted for ${source}: ${documentContext.summary.substring(0, 100)}...`);
                            // Process chunks in batches
                            for (let i = 0; i < chunks.length; i += batchSize) {
                                const batchChunks = chunks.slice(i, i + batchSize);
                                console.log(`Processing batch ${Math.floor(i / batchSize) + 1}/${Math.ceil(chunks.length / batchSize)}`);
                                // Process chunks in parallel
                                const enhancedBatch = await Promise.all(batchChunks.map(chunk => enhanceChunkWithContext(chunk, documentContext)));
                                // Update the original chunks with enhanced ones
                                for (let j = 0; j < batchChunks.length; j++) {
                                    const index = documents.findIndex(d => {
                                        var _a, _b, _c, _d;
                                        return ((_a = d.metadata) === null || _a === void 0 ? void 0 : _a.id) === ((_b = batchChunks[j].metadata) === null || _b === void 0 ? void 0 : _b.id) ||
                                            ((_c = d.metadata) === null || _c === void 0 ? void 0 : _c.chunkId) === ((_d = batchChunks[j].metadata) === null || _d === void 0 ? void 0 : _d.chunkId);
                                    });
                                    if (index !== -1) {
                                        documents[index] = enhancedBatch[j];
                                    }
                                }
                                // Update progress
                                progress.processedDocuments += batchChunks.length;
                                progress.enhancedDocuments += enhancedBatch.filter(c => { var _a; return (_a = c.metadata) === null || _a === void 0 ? void 0 : _a.isContextualChunk; }).length;
                                if (batchProgress) {
                                    batchProgress.processedCount += batchChunks.length;
                                }
                                updateProgress(progress);
                            }
                        }
                        catch (error) {
                            (0, errorHandling_js_1.logError)(`Error processing document: ${source} in batch ${batchId}`, error);
                            progress.errorDocuments += chunks.length;
                            updateProgress(progress);
                        }
                    }
                    // Save updated batch if not simulating
                    if (!SIMULATE) {
                        fs_1.default.writeFileSync(batchPath, JSON.stringify(documents, null, 2));
                        console.log(`Updated batch ${batchId} with contextual embeddings`);
                    }
                    // Update batch status
                    if (batchProgress) {
                        batchProgress.status = 'completed';
                        batchProgress.completedAt = new Date().toISOString();
                        updateProgress(progress);
                    }
                }
                catch (error) {
                    (0, errorHandling_js_1.logError)(`Error processing batch: ${batchId}`, error);
                    // Update batch status
                    if (batchProgress) {
                        batchProgress.status = 'error';
                        batchProgress.error = String(error);
                        updateProgress(progress);
                    }
                }
            }
        }
        // Complete the migration
        progress.completedAt = new Date().toISOString();
        updateProgress(progress);
        console.log('Migration completed successfully!');
        console.log(`Processed ${progress.processedDocuments} documents`);
        console.log(`Enhanced ${progress.enhancedDocuments} chunks with contextual information`);
        console.log(`Encountered errors with ${progress.errorDocuments} documents`);
    }
    catch (error) {
        (0, errorHandling_js_1.logError)('Error during migration', error);
        console.error('Migration failed. See error log for details.');
    }
}
// Run the migration
if (require.main === module) {
    const batchSize = process.argv[2] ? parseInt(process.argv[2]) : DEFAULT_BATCH_SIZE;
    migrateToContextualEmbeddings(batchSize)
        .then(() => {
        console.log('Migration script execution completed');
    })
        .catch(error => {
        console.error('Unhandled error during migration:', error);
        process.exit(1);
    });
}
