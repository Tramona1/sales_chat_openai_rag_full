/**
 * Migration Script for Contextual Embeddings
 * 
 * This script migrates existing documents in the vector store to include
 * contextual information generated by Gemini.
 */

import fs from 'fs';
import path from 'path';
import dotenv from 'dotenv';
import { extractDocumentContext, generateChunkContext } from '../utils/geminiClient.js';
import { getEmbeddingClient } from '../utils/embeddingClient.js';
import { logError, logInfo } from '../utils/errorHandling.js';

// Load environment variables
dotenv.config({ path: path.resolve(process.cwd(), '.env.local') });

// Constants
const VECTOR_STORE_PATH = path.join(process.cwd(), 'data', 'vectorStore.json');
const BATCH_DIR = path.join(process.cwd(), 'data', 'vector_batches');
const MIGRATION_LOG_PATH = path.join(process.cwd(), 'data', 'migration_log.json');

// Configuration
const DEFAULT_BATCH_SIZE = 10;
const SIMULATE = false; // Set to true to simulate without writing changes

// Progress tracking
interface MigrationProgress {
  startedAt: string;
  completedAt?: string;
  totalDocuments: number;
  processedDocuments: number;
  enhancedDocuments: number;
  errorDocuments: number;
  batches: Array<{
    batchId: string;
    documentsCount: number;
    processedCount: number;
    status: 'pending' | 'in-progress' | 'completed' | 'error';
    startedAt?: string;
    completedAt?: string;
    error?: string;
  }>;
}

// Function to create initial progress tracking
function initializeProgress(): MigrationProgress {
  return {
    startedAt: new Date().toISOString(),
    totalDocuments: 0,
    processedDocuments: 0,
    enhancedDocuments: 0,
    errorDocuments: 0,
    batches: []
  };
}

// Function to update progress
function updateProgress(progress: MigrationProgress): void {
  fs.writeFileSync(MIGRATION_LOG_PATH, JSON.stringify(progress, null, 2));
}

// Function to get document batches
function getDocumentBatches(): string[] {
  // Check if batch directory exists
  if (!fs.existsSync(BATCH_DIR)) {
    return [];
  }

  // Get all JSON files in the batch directory
  const files = fs.readdirSync(BATCH_DIR);
  return files
    .filter(file => file.startsWith('batch_') && file.endsWith('.json'))
    .map(file => path.join(BATCH_DIR, file));
}

// Function to load documents from a batch file
function loadBatchDocuments(batchPath: string): any[] {
  try {
    const batchData = fs.readFileSync(batchPath, 'utf8');
    return JSON.parse(batchData);
  } catch (error) {
    logError(`Error loading batch from ${batchPath}`, error);
    return [];
  }
}

// Function to generate context for a document chunk
async function enhanceChunkWithContext(chunk: any, documentContext: any = null): Promise<any> {
  try {
    // Skip if already has context
    if (chunk.metadata?.context && 
        chunk.metadata.documentSummary && 
        chunk.metadata.isContextualChunk) {
      return chunk;
    }

    // Create a new metadata object if it doesn't exist
    if (!chunk.metadata) {
      chunk.metadata = {};
    }

    // Add document context if available
    if (documentContext) {
      chunk.metadata.documentSummary = documentContext.summary;
      chunk.metadata.documentType = documentContext.documentType;
      chunk.metadata.primaryTopics = documentContext.mainTopics.join(', ');
      chunk.metadata.audienceType = documentContext.audienceType.join(', ');
      chunk.metadata.technicalLevel = documentContext.technicalLevel;
    }

    // Generate context for this specific chunk
    const chunkContext = await generateChunkContext(chunk.text, documentContext);
    
    // Add chunk context to metadata
    chunk.metadata.context = chunkContext;
    chunk.metadata.isContextualChunk = true;

    return chunk;
  } catch (error) {
    logError('Error enhancing chunk with context', error);
    return chunk;
  }
}

// Group chunks by source document
function groupChunksBySource(chunks: any[]): { [source: string]: any[] } {
  const groupedChunks: { [source: string]: any[] } = {};
  
  for (const chunk of chunks) {
    const source = chunk.metadata?.source || 'unknown';
    if (!groupedChunks[source]) {
      groupedChunks[source] = [];
    }
    groupedChunks[source].push(chunk);
  }
  
  return groupedChunks;
}

// Main migration function
async function migrateToContextualEmbeddings(batchSize: number = DEFAULT_BATCH_SIZE) {
  console.log('Starting migration to contextual embeddings...');
  
  // Check for Gemini API key
  if (!process.env.GEMINI_API_KEY) {
    console.error('Error: GEMINI_API_KEY is not set in .env.local');
    process.exit(1);
  }

  // Initialize progress tracking
  let progress: MigrationProgress;
  if (fs.existsSync(MIGRATION_LOG_PATH)) {
    // Resume from existing progress
    const progressData = fs.readFileSync(MIGRATION_LOG_PATH, 'utf8');
    progress = JSON.parse(progressData);
    console.log('Resuming migration from previous progress...');
  } else {
    // Start new migration
    progress = initializeProgress();
    updateProgress(progress);
  }

  // Initialize embedding client
  const embeddingClient = getEmbeddingClient();
  console.log(`Using embedding provider: ${embeddingClient.getProvider()}`);

  try {
    // Process the single vector store file if it exists
    if (fs.existsSync(VECTOR_STORE_PATH)) {
      const data = fs.readFileSync(VECTOR_STORE_PATH, 'utf8');
      let vectorStore = JSON.parse(data);
      
      // Handle different vector store formats
      let items = [];
      if (vectorStore.items && Array.isArray(vectorStore.items)) {
        items = vectorStore.items;
      } else if (Array.isArray(vectorStore)) {
        items = vectorStore;
      }
      
      progress.totalDocuments += items.length;
      console.log(`Processing ${items.length} documents from main vector store...`);
      
      // Group chunks by source document
      const groupedChunks = groupChunksBySource(items);
      
      // Process each source document
      for (const [source, chunks] of Object.entries(groupedChunks)) {
        console.log(`Processing document: ${source} with ${chunks.length} chunks`);
        
        try {
          // Build a representative document text for context extraction
          // Combine first chunks up to a reasonable size
          const maxContextSize = 10000;
          let combinedText = '';
          let contentAdded = false;
          
          for (const chunk of chunks) {
            if (combinedText.length < maxContextSize) {
              combinedText += chunk.text + '\n\n';
              contentAdded = true;
            }
          }
          
          if (!contentAdded) {
            console.log(`Skipping empty document: ${source}`);
            continue;
          }
          
          // Extract document context
          console.log(`Extracting context for document: ${source}`);
          const documentContext = await extractDocumentContext(combinedText);
          console.log(`Context extracted for ${source}: ${documentContext.summary.substring(0, 100)}...`);
          
          // Process chunks in batches
          const enhancedChunks = [];
          for (let i = 0; i < chunks.length; i += batchSize) {
            const batchChunks = chunks.slice(i, i + batchSize);
            console.log(`Processing batch ${Math.floor(i/batchSize) + 1}/${Math.ceil(chunks.length/batchSize)}`);
            
            // Process chunks in parallel
            const enhancedBatch = await Promise.all(
              batchChunks.map(chunk => enhanceChunkWithContext(chunk, documentContext))
            );
            
            enhancedChunks.push(...enhancedBatch);
            
            // Update progress
            progress.processedDocuments += batchChunks.length;
            progress.enhancedDocuments += enhancedBatch.filter(c => c.metadata?.isContextualChunk).length;
            updateProgress(progress);
          }
          
          // Replace the original chunks with enhanced ones if not simulating
          if (!SIMULATE) {
            // Find the index positions of the chunks from this source
            for (let i = 0; i < items.length; i++) {
              if (items[i].metadata?.source === source) {
                // Find the corresponding enhanced chunk
                const enhancedChunk = enhancedChunks.find(c => 
                  c.metadata?.id === items[i].metadata?.id ||
                  c.metadata?.chunkId === items[i].metadata?.chunkId
                );
                
                if (enhancedChunk) {
                  items[i] = enhancedChunk;
                }
              }
            }
          }
          
        } catch (error) {
          logError(`Error processing document: ${source}`, error);
          progress.errorDocuments += chunks.length;
          updateProgress(progress);
        }
      }
      
      // Save updated vector store if not simulating
      if (!SIMULATE) {
        if (vectorStore.items) {
          vectorStore.items = items;
        } else {
          vectorStore = items;
        }
        
        // Add last updated timestamp
        if (typeof vectorStore === 'object') {
          vectorStore.lastUpdated = Date.now();
        }
        
        fs.writeFileSync(VECTOR_STORE_PATH, JSON.stringify(vectorStore, null, 2));
        console.log(`Updated main vector store with contextual embeddings`);
      }
    }
    
    // Process batch files if they exist
    const batchPaths = getDocumentBatches();
    if (batchPaths.length > 0) {
      console.log(`Found ${batchPaths.length} batch files to process`);
      
      // Add batch information to progress tracking
      for (const batchPath of batchPaths) {
        const batchId = path.basename(batchPath, '.json');
        
        // Skip if already in progress tracking
        if (!progress.batches.some(b => b.batchId === batchId)) {
          const documents = loadBatchDocuments(batchPath);
          progress.totalDocuments += documents.length;
          
          progress.batches.push({
            batchId,
            documentsCount: documents.length,
            processedCount: 0,
            status: 'pending'
          });
        }
      }
      
      updateProgress(progress);
      
      // Process each batch file
      for (const batchPath of batchPaths) {
        const batchId = path.basename(batchPath, '.json');
        const batchProgress = progress.batches.find(b => b.batchId === batchId);
        
        // Skip completed batches
        if (batchProgress?.status === 'completed') {
          console.log(`Skipping already completed batch: ${batchId}`);
          continue;
        }
        
        console.log(`Processing batch: ${batchId}`);
        
        try {
          // Update batch status
          if (batchProgress) {
            batchProgress.status = 'in-progress';
            batchProgress.startedAt = new Date().toISOString();
            updateProgress(progress);
          }
          
          // Load batch documents
          const documents = loadBatchDocuments(batchPath);
          console.log(`Loaded ${documents.length} documents from batch ${batchId}`);
          
          // Group chunks by source document
          const groupedChunks = groupChunksBySource(documents);
          
          // Process each source document
          for (const [source, chunks] of Object.entries(groupedChunks)) {
            console.log(`Processing document: ${source} with ${chunks.length} chunks`);
            
            try {
              // Build a representative document text for context extraction
              const maxContextSize = 10000;
              let combinedText = '';
              let contentAdded = false;
              
              for (const chunk of chunks) {
                if (combinedText.length < maxContextSize) {
                  combinedText += chunk.text + '\n\n';
                  contentAdded = true;
                }
              }
              
              if (!contentAdded) {
                console.log(`Skipping empty document: ${source}`);
                continue;
              }
              
              // Extract document context
              console.log(`Extracting context for document: ${source}`);
              const documentContext = await extractDocumentContext(combinedText);
              console.log(`Context extracted for ${source}: ${documentContext.summary.substring(0, 100)}...`);
              
              // Process chunks in batches
              for (let i = 0; i < chunks.length; i += batchSize) {
                const batchChunks = chunks.slice(i, i + batchSize);
                console.log(`Processing batch ${Math.floor(i/batchSize) + 1}/${Math.ceil(chunks.length/batchSize)}`);
                
                // Process chunks in parallel
                const enhancedBatch = await Promise.all(
                  batchChunks.map(chunk => enhanceChunkWithContext(chunk, documentContext))
                );
                
                // Update the original chunks with enhanced ones
                for (let j = 0; j < batchChunks.length; j++) {
                  const index = documents.findIndex(d => 
                    d.metadata?.id === batchChunks[j].metadata?.id ||
                    d.metadata?.chunkId === batchChunks[j].metadata?.chunkId
                  );
                  
                  if (index !== -1) {
                    documents[index] = enhancedBatch[j];
                  }
                }
                
                // Update progress
                progress.processedDocuments += batchChunks.length;
                progress.enhancedDocuments += enhancedBatch.filter(c => c.metadata?.isContextualChunk).length;
                
                if (batchProgress) {
                  batchProgress.processedCount += batchChunks.length;
                }
                
                updateProgress(progress);
              }
              
            } catch (error) {
              logError(`Error processing document: ${source} in batch ${batchId}`, error);
              progress.errorDocuments += chunks.length;
              updateProgress(progress);
            }
          }
          
          // Save updated batch if not simulating
          if (!SIMULATE) {
            fs.writeFileSync(batchPath, JSON.stringify(documents, null, 2));
            console.log(`Updated batch ${batchId} with contextual embeddings`);
          }
          
          // Update batch status
          if (batchProgress) {
            batchProgress.status = 'completed';
            batchProgress.completedAt = new Date().toISOString();
            updateProgress(progress);
          }
          
        } catch (error) {
          logError(`Error processing batch: ${batchId}`, error);
          
          // Update batch status
          if (batchProgress) {
            batchProgress.status = 'error';
            batchProgress.error = String(error);
            updateProgress(progress);
          }
        }
      }
    }
    
    // Complete the migration
    progress.completedAt = new Date().toISOString();
    updateProgress(progress);
    
    console.log('Migration completed successfully!');
    console.log(`Processed ${progress.processedDocuments} documents`);
    console.log(`Enhanced ${progress.enhancedDocuments} chunks with contextual information`);
    console.log(`Encountered errors with ${progress.errorDocuments} documents`);
    
  } catch (error) {
    logError('Error during migration', error);
    console.error('Migration failed. See error log for details.');
  }
}

// Run the migration
if (require.main === module) {
  const batchSize = process.argv[2] ? parseInt(process.argv[2]) : DEFAULT_BATCH_SIZE;
  migrateToContextualEmbeddings(batchSize)
    .then(() => {
      console.log('Migration script execution completed');
    })
    .catch(error => {
      console.error('Unhandled error during migration:', error);
      process.exit(1);
    });
}

export { migrateToContextualEmbeddings }; 